{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58235735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#from datetime import timedelta\n",
    "#import calendar\n",
    "import numpy as np\n",
    "#import re\n",
    "#from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff6616",
   "metadata": {},
   "source": [
    "# Load in dataset from assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e921fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Report Type     Id    Class                Submitted Date  \\\n",
      "0            Report  13038  Class A  Saturday, November 12, 2005.   \n",
      "1            Report   8792  Class B       Thursday, June 3, 2004.   \n",
      "2            Report   1255  Class B     Tuesday, October 5, 1999.   \n",
      "3     Media Article    658      NaN   Wednesday, January 23, 2013   \n",
      "4            Report  11616  Class B          Friday, May 6, 2005.   \n",
      "...             ...    ...      ...                           ...   \n",
      "5462         Report   1411  Class A    Tuesday, January 23, 2001.   \n",
      "5463  Media Article     59      NaN      Wednesday, June 18, 1980   \n",
      "5464         Report  14714  Class A         Friday, May 19, 2006.   \n",
      "5465         Report  12884  Class A    Tuesday, October 25, 2005.   \n",
      "5466         Report    798  Class B        Monday, June 26, 2000.   \n",
      "\n",
      "                                               Headline  Year  Season  \\\n",
      "0     Snowmobiler has encounter in deep snow near Po...  2004  Winter   \n",
      "1     Four nocturnal hikers get pelted with snow nea...  2003  Winter   \n",
      "2     Creature observed walking back and forth by wi...  1998    Fall   \n",
      "3               Legendary Bigfoot sighted near Kasigluk   NaN     NaN   \n",
      "4              Fishermen find footprints east of Egegik  2004  Summer   \n",
      "...                                                 ...   ...     ...   \n",
      "5462  Family sees massive ape just after daybreak ne...  1994  Summer   \n",
      "5463          Two Men Report Seeing Huge Hairy Creature   NaN     NaN   \n",
      "5464  Early morning sighting by newspaper deliverer ...  2005  Winter   \n",
      "5465  Daytime sighting by six siesmographic workers ...  1984  Winter   \n",
      "5466  Car passenger sees figure on road on remote, s...  2000  Summer   \n",
      "\n",
      "          Month    State       County  ... PRODUCT_entities PRODUCT_score  \\\n",
      "0      February   Alaska    Anchorage  ...              NaN           NaN   \n",
      "1      December   Alaska    Anchorage  ...              NaN           NaN   \n",
      "2     September   Alaska       Bethel  ...              NaN           NaN   \n",
      "3           NaN      NaN          NaN  ...              NaN           NaN   \n",
      "4          July   Alaska  Bristol Bay  ...              NaN           NaN   \n",
      "...         ...      ...          ...  ...              ...           ...   \n",
      "5462       June  Wyoming        Teton  ...              NaN           NaN   \n",
      "5463        NaN      NaN          NaN  ...              NaN           NaN   \n",
      "5464   November  Wyoming        Uinta  ...              NaN           NaN   \n",
      "5465    January  Wyoming        Uinta  ...              NaN           NaN   \n",
      "5466       June  Wyoming     Washakie  ...              NaN           NaN   \n",
      "\n",
      "     EVENT_entities EVENT_score MONEY_entities LAW_entities MONEY_score  \\\n",
      "0               NaN         NaN            NaN          NaN         NaN   \n",
      "1               NaN         NaN            NaN          NaN         NaN   \n",
      "2               NaN         NaN            NaN          NaN         NaN   \n",
      "3               NaN         NaN            NaN          NaN         NaN   \n",
      "4               NaN         NaN            NaN          NaN         NaN   \n",
      "...             ...         ...            ...          ...         ...   \n",
      "5462            NaN         NaN            NaN          NaN         NaN   \n",
      "5463    a \"Big Foot         0.2            NaN          NaN         NaN   \n",
      "5464            NaN         NaN            NaN          NaN         NaN   \n",
      "5465            NaN         NaN            NaN          NaN         NaN   \n",
      "5466            NaN         NaN            NaN          NaN         NaN   \n",
      "\n",
      "     LAW_score LANGUAGE_entities LANGUAGE_score  \n",
      "0          NaN               NaN            NaN  \n",
      "1          NaN               NaN            NaN  \n",
      "2          NaN               NaN            NaN  \n",
      "3          NaN               NaN            NaN  \n",
      "4          NaN               NaN            NaN  \n",
      "...        ...               ...            ...  \n",
      "5462       NaN               NaN            NaN  \n",
      "5463       NaN               NaN            NaN  \n",
      "5464       NaN               NaN            NaN  \n",
      "5465       NaN               NaN            NaN  \n",
      "5466       NaN               NaN            NaN  \n",
      "\n",
      "[5467 rows x 119 columns]\n"
     ]
    }
   ],
   "source": [
    "# read in tsv dataset\n",
    "reports = pd.read_csv('../../Data/reports_v3.tsv', sep='\\t')\n",
    "\n",
    "print(reports)\n",
    "\n",
    "# get an overview of all columsn\n",
    "# for col in reports.columns:\n",
    "    # print(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84122412",
   "metadata": {},
   "source": [
    "# Selecting columns of interest and preparing for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "921775b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      National Park Visitation Count  BigFoot Hotspot  Year\n",
      "0                             244232            False  2004\n",
      "1                             243719            False  2003\n",
      "2                               5550            False  1998\n",
      "4                              56787            False  2004\n",
      "5                              28331            False  2000\n",
      "...                              ...              ...   ...\n",
      "5461                         2590624            False  2000\n",
      "5462                         2540699            False  1994\n",
      "5464                         2463442            False  2005\n",
      "5465                         1360898            False  1984\n",
      "5466                         2838233            False  2000\n",
      "\n",
      "[4200 rows x 3 columns]\n",
      "    Year  Non_Hotspots_Median  Hotspots_Median\n",
      "0   1979            1118800.0         963726.0\n",
      "1   1980            1160588.0        1268256.0\n",
      "2   1981            1286316.0        1233671.0\n",
      "3   1982            1015580.0        1007300.0\n",
      "4   1983            1133157.0        1106306.0\n",
      "5   1984            1501450.0        1152411.0\n",
      "6   1985            1112764.0        1165640.0\n",
      "7   1986            1042148.0        1298457.0\n",
      "8   1987            1113405.0        1292027.0\n",
      "9   1988            1229717.0         677135.0\n",
      "10  1989            1672840.0        1004208.5\n",
      "11  1990            1123175.0        1327101.0\n",
      "12  1991            1358984.0        1549412.0\n",
      "13  1992            1504013.0         468011.0\n",
      "14  1993            1763094.0        1365213.0\n",
      "15  1994            1528537.0        1426244.0\n",
      "16  1995            1617077.0        1438227.0\n",
      "17  1996            1538105.0        1338961.0\n",
      "18  1997            1509596.0        1315773.0\n",
      "19  1998            1473100.0        1353793.0\n",
      "20  1999            1384469.0        1291397.0\n",
      "21  2000            1419579.0        1344833.0\n",
      "22  2001            1296786.0        1301103.0\n",
      "23  2002            1389244.0        1310390.0\n",
      "24  2003            1272848.5        1262351.0\n",
      "25  2004            1418735.0        1217750.0\n",
      "26  2005            1340160.0        1173897.0\n",
      "27  2006            1113601.0        1113601.0\n",
      "28  2007            1107227.0        1047685.0\n",
      "29  2008            1212854.0        1163227.0\n",
      "30  2009            1151654.0        1151654.0\n",
      "31  2010            2150345.0        1191754.0\n",
      "32  2011            1209883.0        1038229.0\n",
      "33  2012            1210200.0        1049178.0\n",
      "34  2013            1071823.0         896600.0\n",
      "35  2014            1264259.0        1264259.0\n",
      "36  2015            1640195.0        1237231.0\n",
      "37  2016            1437341.0        1356913.0\n",
      "38  2017            1415867.0        1415867.0\n",
      "39  2018            1264880.0        2311473.0\n",
      "40  2019            2055309.0        1003171.5\n",
      "41  2020            1348215.0        1160754.0\n",
      "42  2021            1682720.0         435879.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tberg_cl8rsaz\\AppData\\Local\\Temp\\ipykernel_14288\\99739560.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# define a list of only the columns I need for my visualization\n",
    "columns_of_interest = ['National Park Visitation Count', 'BigFoot Hotspot', 'Year']\n",
    "\n",
    "# keep only the columns of interest in the datase\n",
    "df = reports[columns_of_interest]\n",
    "\n",
    "#print(df)\n",
    "\n",
    "# inspect value counts and nan values\n",
    "#print(df['Year'].value_counts())\n",
    "#print(df['BigFoot Hotspot'].value_counts())\n",
    "#print(df['National Park Visitation Count'].value_counts())\n",
    "#print(df['National Park Visitation Count'].isna().sum())\n",
    "#print(df['BigFoot Hotspot'].isna().sum())\n",
    "#print(df['Year'].isna().sum())\n",
    "\n",
    "# dropping nan rows (these only exist for national park visiation count)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# dropping rows where the year is 0\n",
    "df = df[df['Year'] != 0]\n",
    "\n",
    "# converting the national park visitation count to integer (currently float)\n",
    "df['National Park Visitation Count'] = df['National Park Visitation Count'].astype(int)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# print(df['Year'].value_counts())\n",
    "\n",
    "# Group the data by 'Year' and 'BigFoot Hotspot' and compute the median visitation count\n",
    "median_visitation_counts = df.groupby(['Year', 'BigFoot Hotspot'])['National Park Visitation Count'].median().reset_index()\n",
    "\n",
    "# Pivot the data to have years as rows and columns for false and true hotspots\n",
    "pivot_table = median_visitation_counts.pivot(index='Year', columns='BigFoot Hotspot', values='National Park Visitation Count')\n",
    "\n",
    "# Drop rows with missing values\n",
    "pivot_table.dropna(inplace=True)\n",
    "\n",
    "# Rename the columns for clarity\n",
    "pivot_table.columns = ['Non_Hotspots_Median', 'Hotspots_Median']\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#visitation_counts_1 = pivot_table['Non_Hotspots_Median'].values.reshape(-1, 1)\n",
    "#visitation_counts_1 = pivot_table['Hotspots_Median'].values.reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the visitation counts using MinMaxScaler\n",
    "#pivot_table['Non_Hotspots_Median'] = scaler.fit_transform(visitation_counts_1)\n",
    "#pivot_table['Hotspots_Median'] = scaler.fit_transform(visitation_counts_1)\n",
    "\n",
    "# adding year column in (currently the index)\n",
    "pivot_table.reset_index(inplace=True)\n",
    "\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c45152",
   "metadata": {},
   "source": [
    "# Convert to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e95f77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to a dictionary of records and wrap in another dictionary\n",
    "dic = {'BFRO': pivot_table.to_dict(orient='records')}\n",
    "\n",
    "# Write the dictionary to a JSON file with pretty printing\n",
    "with open('../../Dataset1/BFRO_vis_1.json', 'w') as f:\n",
    "    json.dump(dic, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
